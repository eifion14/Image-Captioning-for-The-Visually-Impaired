{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Z2K5r0vdZcr",
        "outputId": "acdb9124-fc2c-4b4c-b0a3-156cdc8f8c70"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# from google.colab import auth\n",
        "# auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6QMyhNBUoum"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(VIZWIZ_CAPTION_DIR)\n",
        "\n",
        "from vizwiz_api.vizwiz import VizWiz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEMgEkrtdu7W",
        "outputId": "d7b0c2a8-b25e-4e80-e04e-bcd1ffbf097c"
      },
      "outputs": [],
      "source": [
        "%cd VIZWIZ_CAPTION_DIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5A2A1F8Vp0m"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re, string, unicodedata\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzx2IwSsPzfa",
        "outputId": "e2e240ac-b328-49db-d0b6-bae55781c272"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7Od0SdxHzNY"
      },
      "outputs": [],
      "source": [
        "DATA_DIR = VIZWIZ_CAPTION_DIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3SMrn3AVezY"
      },
      "outputs": [],
      "source": [
        "ANN_TRAIN = \"annotations/train.json\"\n",
        "ANN_VAL   = \"annotations/val.json\"\n",
        "ANN_TEST  = \"annotations/test.json\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GoEMiKmdUZEK",
        "outputId": "7f57bcd9-3279-4d39-b218-125f0741a12e"
      },
      "outputs": [],
      "source": [
        "# load via VizWiz API\n",
        "vizwiz_train = VizWiz(ANN_TRAIN, ignore_rejected=True, ignore_precanned=True)\n",
        "vizwiz_val   = VizWiz(ANN_VAL,   ignore_rejected=True, ignore_precanned=True)\n",
        "# For test, these flags don’t matter (there are no captions), but we keep the call uniform:\n",
        "# vw_test  = VizWiz(ANN_TEST,  ignore_rejected=True, ignore_precanned=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLZoaoeHaW_d"
      },
      "outputs": [],
      "source": [
        "# 23431*5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZJeVqmLVnpQ"
      },
      "outputs": [],
      "source": [
        "train_anns = pd.DataFrame(vizwiz_train.dataset[\"annotations\"])\n",
        "val_anns   = pd.DataFrame(vizwiz_val.dataset[\"annotations\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gl48oNgzYXhW",
        "outputId": "35c904b2-de6d-4a8d-9a38-5d1a7ce6d97d"
      },
      "outputs": [],
      "source": [
        "train_anns.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkS1_QGMYbyd",
        "outputId": "8d45bd9f-eac7-47a2-fae9-b9efd05f5ac6"
      },
      "outputs": [],
      "source": [
        "val_anns.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "9McupxMYYgI3",
        "outputId": "9e573975-5e38-4d8e-e193-b86521dcfd86"
      },
      "outputs": [],
      "source": [
        "train_anns.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "op3cuJ-8b50K"
      },
      "outputs": [],
      "source": [
        "train_imgs = pd.DataFrame(vizwiz_train.dataset[\"images\"]).rename(columns={\"id\":\"image_id\"})[[\"image_id\",\"file_name\"]]\n",
        "\n",
        "# keep images with >= 1 caption\n",
        "train_anns_ = train_anns[[\"image_id\",\"caption\"]].dropna()\n",
        "df_train_grouped = (\n",
        "    train_anns_\n",
        "    .groupby(\"image_id\", as_index=False)[\"caption\"].agg(list)   # list of captions per image\n",
        "    .merge(train_imgs, on=\"image_id\", how=\"inner\")\n",
        ")\n",
        "df_train_grouped = df_train_grouped[df_train_grouped[\"caption\"].map(len) > 0].reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Ar0NB69dpTm",
        "outputId": "d6685786-4724-4d85-847b-050c5cb1d0c6"
      },
      "outputs": [],
      "source": [
        "df_train_grouped.iloc[0][\"caption\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jr27kv6-dggB"
      },
      "outputs": [],
      "source": [
        "val_imgs   = pd.DataFrame(vizwiz_val.dataset[\"images\"]).rename(columns={\"id\":\"image_id\"})[[\"image_id\",\"file_name\"]]\n",
        "\n",
        "val_anns_any = val_anns[[\"image_id\",\"caption\"]].dropna()\n",
        "df_val_grouped = (\n",
        "    val_anns_any\n",
        "    .groupby(\"image_id\", as_index=False)[\"caption\"].agg(list)\n",
        "    .merge(val_imgs, on=\"image_id\", how=\"inner\")\n",
        ").reset_index(drop=True)\n",
        "\n",
        "#  “exactly 5 captions” – recommended for metrics to match common protocols\n",
        "df_val_grouped = df_val_grouped[df_val_grouped[\"caption\"].map(len) == 5].reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFcQr3CBeFeD",
        "outputId": "ce026bdb-039c-4d33-ccef-4c6d8bad26b1"
      },
      "outputs": [],
      "source": [
        "df_val_grouped.iloc[0][\"caption\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YeyVBgj5f1Zd"
      },
      "outputs": [],
      "source": [
        "def clean_caption(\n",
        "    text,\n",
        "    lower = True,\n",
        "    normalize_unicode = True,\n",
        "    strip_punct = True,\n",
        "    keep_nums = True,\n",
        "    keep_single_letters = False, #\n",
        "    keep_a_and_i = True,\n",
        "    keep_stopwords = True,\n",
        "    collapse_whitespace = True,\n",
        "    min_tokens = 1\n",
        "):\n",
        "    t = text\n",
        "    if t is None:\n",
        "        return \"\"\n",
        "    if lower:\n",
        "        t = t.lower()\n",
        "    if normalize_unicode:\n",
        "        t = unicodedata.normalize(\"NFKC\", t)\n",
        "    if strip_punct:\n",
        "        t = t.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "    if not keep_nums:\n",
        "        t = re.sub(r\"\\d+\", \"\", t)\n",
        "\n",
        "    tokens = t.split()\n",
        "    if not keep_single_letters:\n",
        "        if keep_a_and_i:\n",
        "            tokens = [w for w in tokens if (len(w) > 1) or (w in [\"a\", \"i\"])]\n",
        "        else:\n",
        "            tokens = [w for w in tokens if len(w) > 1]\n",
        "\n",
        "    if not keep_stopwords:\n",
        "        stopwords = set(nltk.corpus.stopwords.words(\"english\"))\n",
        "        tokens = [w for w in tokens if w not in stopwords]\n",
        "\n",
        "    if collapse_whitespace:\n",
        "        t = \" \".join(tokens).strip()\n",
        "    else:\n",
        "        t = \" \".join(tokens)\n",
        "\n",
        "    if min_tokens > 0:\n",
        "        if len(t.split()) < min_tokens:\n",
        "            t = \"\"\n",
        "\n",
        "    return t\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8pZUNC1YK-JD"
      },
      "outputs": [],
      "source": [
        "df_train_grouped[\"caption_clean\"] = df_train_grouped[\"caption\"].map(lambda x: [clean_caption(c) for c in x])\n",
        "df_val_grouped[\"caption_clean\"] = df_val_grouped[\"caption\"].map(lambda x: [clean_caption(c) for c in x])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aa7Otz87ZGAy"
      },
      "outputs": [],
      "source": [
        "# start from grouped with columns: image_id, file_name, caption_clean (list)\n",
        "df_train_long = (\n",
        "    df_train_grouped[[\"image_id\",\"file_name\",\"caption_clean\"]]\n",
        "    .explode(\"caption_clean\", ignore_index=True)\n",
        ")\n",
        "\n",
        "df_val_long = (\n",
        "    df_val_grouped[[\"image_id\",\"file_name\",\"caption_clean\"]]\n",
        "    .explode(\"caption_clean\", ignore_index=True)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 640
        },
        "id": "AuSkCMCZPnrK",
        "outputId": "20d3a9db-8cdf-4be7-9f55-92c97155eaaa"
      },
      "outputs": [],
      "source": [
        "# choosing best caption length for computaiton cost vs relevance\n",
        "def length_stats(df, col=\"caption_clean\"):\n",
        "    lens = df[col].str.split().map(len)\n",
        "    out = pd.Series({\n",
        "        \"count\": int(lens.size),\n",
        "        \"min\":   int(lens.min()),\n",
        "        \"max\":   int(lens.max()),\n",
        "        \"mean\":  float(lens.mean()),\n",
        "        \"std\":   float(lens.std()),\n",
        "        \"median\":float(lens.median()),\n",
        "        \"p90\":   int(lens.quantile(0.90)),\n",
        "        \"p95\":   int(lens.quantile(0.95)),\n",
        "        \"p99\":   int(lens.quantile(0.99)),\n",
        "    })\n",
        "    return lens, out\n",
        "\n",
        "train_lens, train_stats = length_stats(df_train_long)\n",
        "print(train_stats)\n",
        "vc_train = train_lens.value_counts().sort_index()\n",
        "\n",
        "plt.figure()\n",
        "vc_train.plot(kind=\"bar\")\n",
        "plt.title(\"caption length counts\")\n",
        "plt.xlabel(\"words per caption\")\n",
        "plt.xlim(1,30)\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4G_F1u34UUYd"
      },
      "outputs": [],
      "source": [
        "max_len_text = 20 + 2 # + 2 for start and end tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1u-icMvNWLlA",
        "outputId": "c97e1d63-395b-4c7a-bb50-0706632f5230"
      },
      "outputs": [],
      "source": [
        "# setup for removing words that arent frequent across all captions\n",
        "\n",
        "# one word per row\n",
        "train_words = df_train_long[\"caption_clean\"].str.split().explode().dropna().astype(str)\n",
        "\n",
        "# frequency of each word\n",
        "freq = train_words.value_counts()\n",
        "\n",
        "# quick stats over \"counts per word\"\n",
        "vocab_stats = pd.Series({\n",
        "    \"unique_words\": int(freq.size),\n",
        "    \"total_tokens\": int(freq.sum()),\n",
        "    \"mean_per_word\": float(freq.mean()),\n",
        "    \"median_per_word\": float(freq.median()),\n",
        "    \"p90_per_word\": float(freq.quantile(0.90)),\n",
        "    \"p95_per_word\": float(freq.quantile(0.95)),\n",
        "    \"p99_per_word\": float(freq.quantile(0.99)),\n",
        "})\n",
        "print(vocab_stats)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "mVw6RpJdWLFn",
        "outputId": "890c2591-fcd5-4ff1-fad8-96763343b9b6"
      },
      "outputs": [],
      "source": [
        "thresholds = [i for i in range(1, 13)]\n",
        "summary = pd.DataFrame({\n",
        "    \"min_count\": thresholds,\n",
        "    \"vocab_size\": [(freq >= t).sum() for t in thresholds],\n",
        "    \"coverage_tokens_%\": [100 * freq[freq >= t].sum() / freq.sum() for t in thresholds],\n",
        "})\n",
        "summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BbZvKGlNbhHm"
      },
      "outputs": [],
      "source": [
        "min_word_count = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vdD5XJTVtzV"
      },
      "outputs": [],
      "source": [
        "keep_vocab = set(freq[freq >= min_word_count].index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QaPpvogcbf_t"
      },
      "outputs": [],
      "source": [
        "def map_rare_words_to_unk(s):\n",
        "    return \" \".join(w if w in keep_vocab else \"<unk>\" for w in s.split())\n",
        "\n",
        "# new columns with mapped text\n",
        "df_train_long[\"caption_mapped\"] = df_train_long[\"caption_clean\"].map(map_rare_words_to_unk)\n",
        "df_val_long[\"caption_mapped\"]   = df_val_long[\"caption_clean\"].map(map_rare_words_to_unk)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rq119dcjbzOM",
        "outputId": "f334950e-c711-430d-cb6d-3df2eb81be80"
      },
      "outputs": [],
      "source": [
        "unk_rate = (df_train_long[\"caption_mapped\"].str.split().explode().eq(\"<unk>\").mean())\n",
        "print(f\"UNK rate (train): {unk_rate:.2%}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWWH7t6e5l_N"
      },
      "outputs": [],
      "source": [
        "# ends up lowering vocab from 18957 to ~4690, only lose ~3% of total words for ~/4 vocab size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crUCMzKbeFJY"
      },
      "outputs": [],
      "source": [
        "START, END = \"startcap\", \"endcap\"\n",
        "\n",
        "df_train_long[\"caption_final\"] = START + \" \" + df_train_long[\"caption_mapped\"] + \" \" + END\n",
        "df_val_long[\"caption_final\"]   = START + \" \" + df_val_long[\"caption_mapped\"]   + \" \" + END\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSDUNfM3eZLq",
        "outputId": "66582229-e5ac-4ad8-86a3-00f229026a41"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(oov_token=\"<unk>\", filters=\"\")  # don't filter; we already cleaned\n",
        "tokenizer.fit_on_texts(df_train_long[\"caption_final\"])\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "start_id = tokenizer.word_index[START]\n",
        "end_id   = tokenizer.word_index[END]\n",
        "unk_id   = tokenizer.word_index[\"<unk>\"]\n",
        "print(\"vocab_size:\", vocab_size, \"| ids:\", {\"startcap\": start_id, \"endcap\": end_id, \"<unk>\": unk_id})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-v6wLAHXedN_"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def texts_to_padded(texts, tok, end_id, max_len_total):\n",
        "    seqs = tok.texts_to_sequences(texts)\n",
        "    fixed = []\n",
        "    for s in seqs:\n",
        "        # remove the existing end if present\n",
        "        if s and s[-1] == end_id:\n",
        "            core = s[:-1]\n",
        "        else:\n",
        "            core = s\n",
        "        # keep room for exactly one END\n",
        "        core = core[:max_len_total - 1]\n",
        "        s = core + [end_id]\n",
        "        fixed.append(s)\n",
        "    return pad_sequences(fixed, maxlen=max_len_total, padding=\"post\", truncating=\"post\")\n",
        "\n",
        "\n",
        "X_train = texts_to_padded(df_train_long[\"caption_final\"].tolist(), tokenizer, end_id, max_len_text)\n",
        "X_val   = texts_to_padded(df_val_long[\"caption_final\"].tolist(), tokenizer, end_id, max_len_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8yMTudrga7k",
        "outputId": "babbc4ba-123e-4fa1-a57d-c80f4d269adf"
      },
      "outputs": [],
      "source": [
        "seq = X_train[0] # shape: (max_len_text,)\n",
        "print(\"IDS:\", seq.tolist())\n",
        "\n",
        "# detokenize (Tokenizer ignores 0 pads)\n",
        "print(\"TEXT:\", tokenizer.sequences_to_texts([seq])[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMJlc3hAm-0I",
        "outputId": "25046b41-e152-4520-d3c0-564373edc40b"
      },
      "outputs": [],
      "source": [
        "## vocab changed? padding did this?\n",
        "print(vocab_size)\n",
        "print(X_train.max() + 1)\n",
        "vocab_size = X_train.max() + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9wuknVhlm_EY"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxohu147ocQ_"
      },
      "outputs": [],
      "source": [
        "def build_encoder(img_size=299, feat_layer=\"mixed7\", proj_dim=512, freeze=True):\n",
        "\n",
        "    # Input layer for the image\n",
        "    image_input = layers.Input(shape=(img_size, img_size, 3), name=\"image_input\")\n",
        "\n",
        "    # Load InceptionV3 base model pre-trained on ImageNet\n",
        "    base = tf.keras.applications.InceptionV3(include_top=False, weights=\"imagenet\", input_tensor=image_input)\n",
        "\n",
        "    # Freeze base model layers if specified\n",
        "    if freeze:\n",
        "        for l in base.layers:\n",
        "            l.trainable = False\n",
        "\n",
        "    # Get features from the specified layer\n",
        "    feat_map = base.get_layer(feat_layer).output\n",
        "\n",
        "    # Project features to `proj_dim`\n",
        "    proj = layers.Conv2D(proj_dim, 1, activation=\"relu\", name=\"enc_proj\")(feat_map)\n",
        "\n",
        "    # Reshape features into a sequence\n",
        "    s = proj.shape[1] * proj.shape[2]  # H*W\n",
        "    enc_seq = layers.Reshape((s, proj_dim), name=\"encoder_seq\")(proj)  # (B, S, D)\n",
        "\n",
        "    return Model(image_input, enc_seq, name=\"encoder\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_y5lYmEspQM7"
      },
      "outputs": [],
      "source": [
        "def build_decoder(vocab_size, max_len_text, emb_dim=256, rnn_units=512):\n",
        "    # Inputs\n",
        "    features_in = layers.Input(shape=(None, 512), name=\"features_in\")   # (B, S, D)\n",
        "    dec_in = layers.Input(shape=(max_len_text-1,), name=\"decoder_in\")  # (B, T)\n",
        "\n",
        "    # Trainable word embeddings (Option 1) # mask_zero true already handled, error if true\n",
        "    x = layers.Embedding(vocab_size, emb_dim, mask_zero=False, name=\"word_embedding\")(dec_in)  # (B, T, E)\n",
        "\n",
        "    # Decoder RNN (returns sequence)\n",
        "    h_seq = layers.GRU(rnn_units, return_sequences=True, name=\"decoder_gru\")(x)               # (B, T, H)\n",
        "\n",
        "    # Bahdanau/Additive attention: queries=h_seq, values=features\n",
        "    # Keras AdditiveAttention handles (B, T, H) queries vs (B, S, D) values via projection.\n",
        "    context = layers.AdditiveAttention(name=\"bahdanau_attn\")([h_seq, features_in])            # (B, T, D)\n",
        "\n",
        "    # Fuse decoder state + context, then project to vocab per timestep\n",
        "    fuse = layers.Concatenate(name=\"fuse\")([h_seq, context])                                   # (B, T, H+D)\n",
        "    # old softmax\n",
        "    # hidden = layers.TimeDistributed(layers.Dense(rnn_units, activation=\"relu\"), name=\"td_dense\")(fuse)\n",
        "    # logits = layers.TimeDistributed(layers.Dense(vocab_size, activation=\"softmax\"), name=\"td_softmax\")(hidden)\n",
        "    hidden = layers.TimeDistributed(layers.Dense(rnn_units, activation=\"relu\"), name=\"td_dense\")(fuse)\n",
        "    logits = layers.TimeDistributed(layers.Dense(vocab_size), name=\"td_logits\")(hidden)\n",
        "\n",
        "\n",
        "    return Model([features_in, dec_in], logits, name=\"decoder\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zRx4pXnTptra",
        "outputId": "87ad4449-835f-47ff-832b-a17e1ffcd619"
      },
      "outputs": [],
      "source": [
        "img_size = 299\n",
        "encoder = build_encoder(img_size=img_size, feat_layer=\"mixed7\", proj_dim=512, freeze=True)\n",
        "decoder = build_decoder(vocab_size=vocab_size, max_len_text=max_len_text, emb_dim=256, rnn_units=512)\n",
        "\n",
        "img_in  = encoder.input\n",
        "feats   = encoder.output # (B, S, 512)\n",
        "dec_in  = layers.Input(shape=(max_len_text-1,), name=\"decoder_in\")  # (B, T)\n",
        "y_seq   = decoder([feats, dec_in]) # (B, T, V)\n",
        "\n",
        "model = Model([img_in, dec_in], y_seq, name=\"caption_model\")\n",
        "\n",
        "model.encoder=encoder\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=2e-4, clipnorm=1.0),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",
        ")\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_AYqV9Twv90"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "import os\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Placeholder for your GCS bucket name\n",
        "GCS_PROJECT_ID = \"your-gcp-project-id\"\n",
        "GCS_BUCKET_NAME = \"your-gcs-bucket-name\"\n",
        "\n",
        "# Update project config\n",
        "!gcloud config set project {GCS_PROJECT_ID}\n",
        "\n",
        "# Local folders on the Colab VM (placeholders)\n",
        "LOCAL_TRAIN_IMAGES_DIR = \"/content/train\"\n",
        "LOCAL_VAL_IMAGES_DIR   = \"/content/val\"\n",
        "\n",
        "!mkdir -p \"{LOCAL_TRAIN_IMAGES_DIR}\"\n",
        "!mkdir -p \"{LOCAL_VAL_IMAGES_DIR}\"\n",
        "\n",
        "# Sync once from GCS → local (using placeholder bucket and local dirs)\n",
        "TRAIN_IMAGE_ROOT = f\"gs://{GCS_BUCKET_NAME}/train\"\n",
        "VAL_IMAGE_ROOT   = f\"gs://{GCS_BUCKET_NAME}/val\"\n",
        "\n",
        "!gsutil -m rsync -r \"{TRAIN_IMAGE_ROOT}\" \"{LOCAL_TRAIN_IMAGES_DIR}\"\n",
        "!gsutil -m rsync -r \"{VAL_IMAGE_ROOT}\"   \"{LOCAL_VAL_IMAGES_DIR}\"\n",
        "\n",
        "# Now override roots to point to local copies\n",
        "TRAIN_IMAGE_ROOT = LOCAL_TRAIN_IMAGES_DIR\n",
        "VAL_IMAGE_ROOT   = LOCAL_VAL_IMAGES_DIR\n",
        "\n",
        "train_paths = (TRAIN_IMAGE_ROOT + \"/\" + df_train_long[\"file_name\"]).to_numpy()\n",
        "val_paths   = (VAL_IMAGE_ROOT   + \"/\" + df_val_long[\"file_name\"]).to_numpy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mk4xnNixx7tL",
        "outputId": "abb35362-765c-496e-d4c3-5991c42759ec"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "IMG_SIZE = 299\n",
        "from tensorflow.keras.applications.inception_v3 import preprocess_input as iv3_preprocess\n",
        "# save image\n",
        "\n",
        "\n",
        "def preprocess_image(path):\n",
        "    img = tf.io.read_file(path)                    # works with gs://\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize_with_pad(img, IMG_SIZE, IMG_SIZE)\n",
        "    img = tf.cast(img, tf.float32)\n",
        "    return iv3_preprocess(img)                     # InceptionV3 preprocessing\n",
        "\n",
        "# full caption length (incl. [start] and [end])\n",
        "T_total = max_len_text\n",
        "T = T_total - 1   # decoder timesteps\n",
        "\n",
        "def load_and_pack(path, seq):\n",
        "    \"\"\"\n",
        "    seq is shape (T_total,) = [start, w1, ..., w_{T-1}, end, 0, 0, ...]\n",
        "    We build:\n",
        "      dec_in  = [start, w1, ..., w_{T-1}]   (T,)\n",
        "      dec_tgt = [w1, ..., w_{T-1}, end]     (T,)\n",
        "    \"\"\"\n",
        "    img = preprocess_image(path)\n",
        "    dec_in  = seq[:-1]    # (T,)\n",
        "    dec_tgt = seq[1:]     # (T,)\n",
        "    return (img, dec_in), dec_tgt\n",
        "\n",
        "def make_dataset(paths, X, batch_size=64, shuffle=True, cache_path=None):\n",
        "    ds = tf.data.Dataset.from_tensor_slices((paths, X))\n",
        "\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(\n",
        "            buffer_size=min(len(X), 8192),\n",
        "            reshuffle_each_iteration=True\n",
        "        )\n",
        "\n",
        "    ds = ds.map(\n",
        "        load_and_pack,\n",
        "        num_parallel_calls=tf.data.AUTOTUNE,\n",
        "        deterministic=False\n",
        "    )\n",
        "\n",
        "    ds = ds.batch(batch_size)\n",
        "\n",
        "    if cache_path is not None:\n",
        "        # Delete the lockfile if it exists before caching\n",
        "        lockfile = cache_path + '_0.lockfile'\n",
        "        if os.path.exists(lockfile):\n",
        "            print(f\"Deleting old cache lockfile: {lockfile}\")\n",
        "            os.remove(lockfile)\n",
        "        ds = ds.cache(cache_path)   # first epoch fills, later epochs are fast\n",
        "\n",
        "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "# 1) Build datasets with cache enabled\n",
        "train_ds = make_dataset(\n",
        "    train_paths, X_train,\n",
        "    batch_size=64,\n",
        "    shuffle=True,\n",
        "    cache_path=LOCAL_TRAIN_CACHE_PATH,\n",
        ")\n",
        "\n",
        "val_ds = make_dataset(\n",
        "    val_paths, X_val,\n",
        "    batch_size=64,\n",
        "    shuffle=False,\n",
        "    cache_path=LOCAL_VAL_CACHE_PATH,\n",
        ")\n",
        "\n",
        "# 2) FORCE a full pass over train/val to fill the caches\n",
        "for _ in train_ds:\n",
        "    pass\n",
        "\n",
        "for _ in val_ds:\n",
        "    pass\n",
        "\n",
        "print(\"Caches warmed.\")\n",
        "\n",
        "# 3) (Optional but clean) Rebuild datasets that reuse the same cache files\n",
        "train_ds = make_dataset(\n",
        "    train_paths, X_train,\n",
        "    batch_size=64,\n",
        "    shuffle=True,\n",
        "    cache_path=LOCAL_TRAIN_CACHE_PATH,\n",
        ")\n",
        "\n",
        "val_ds = make_dataset(\n",
        "    val_paths, X_val,\n",
        "    batch_size=64,\n",
        "    shuffle=False,\n",
        "    cache_path=LOCAL_VAL_CACHE_PATH,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHKmXAGct46I",
        "outputId": "74675261-197a-4c26-ce7c-dd5689e74237"
      },
      "outputs": [],
      "source": [
        "# Take one batch from the training dataset to inspect tensor shapes\n",
        "for (imgs, dec_in), dec_tgt in train_ds.take(1):\n",
        "    print(\"imgs   :\", imgs.shape)\n",
        "    print(\"dec_in :\", dec_in.shape)\n",
        "    print(\"dec_tgt:\", dec_tgt.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qp6QwxRDsmX4"
      },
      "outputs": [],
      "source": [
        "MODEL_DIR = f\"gs://{GCS_BUCKET_NAME}/vizwiz_models/v1\"\n",
        "\n",
        "from tensorflow.keras import callbacks\n",
        "\n",
        "# file patterns on GCS\n",
        "CKPT_BEST = MODEL_DIR + \"/best_val_loss.weights.h5\"\n",
        "CKPT_EPOCH = MODEL_DIR + \"/epoch_{epoch:02d}_val{val_loss:.3f}.weights.h5\"\n",
        "\n",
        "checkpoint_best = callbacks.ModelCheckpoint(\n",
        "    CKPT_BEST,\n",
        "    save_weights_only=True,\n",
        "    save_best_only=True,\n",
        "    monitor=\"val_loss\",\n",
        "    mode=\"min\",\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "checkpoint_every_epoch = callbacks.ModelCheckpoint(\n",
        "    CKPT_EPOCH,\n",
        "    save_weights_only=True,\n",
        "    save_best_only=False,   # save EVERY epoch\n",
        "    monitor=\"val_loss\",    \n",
        "    mode=\"min\",\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "reduce_lr = callbacks.ReduceLROnPlateau(\n",
        "    monitor=\"val_loss\", factor=0.5, patience=2, verbose=1\n",
        ")\n",
        "\n",
        "early_stop = callbacks.EarlyStopping(\n",
        "    monitor=\"val_loss\", patience=4, restore_best_weights=True, verbose=1\n",
        ")\n",
        "\n",
        "class UnfreezeFrom(callbacks.Callback):\n",
        "    def __init__(self, start_from_layer=\"mixed7\", at_epoch=2):\n",
        "        super().__init__()\n",
        "        self.start_from_layer = start_from_layer\n",
        "        self.at_epoch = at_epoch\n",
        "        self.already_unfrozen = False\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        # Do nothing after first unfreeze\n",
        "        if self.already_unfrozen:\n",
        "            return\n",
        "\n",
        "        if epoch == self.at_epoch:\n",
        "            enc = self.model.encoder \n",
        "            start = False\n",
        "            n = 0\n",
        "\n",
        "            for l in enc.layers:\n",
        "                if l.name == self.start_from_layer:\n",
        "                    start = True\n",
        "                if start:\n",
        "                    if isinstance(l, tf.keras.layers.BatchNormalization):\n",
        "                        l.trainable = False\n",
        "                    else:\n",
        "                        l.trainable = True\n",
        "                        n += 1\n",
        "\n",
        "            self.already_unfrozen = True\n",
        "            print(f\"[Unfreeze] {n} layers unfrozen from '{self.start_from_layer}' upward (BN frozen).\")\n",
        "\n",
        "unfreeze_cb = UnfreezeFrom(start_from_layer=\"mixed7\", at_epoch=2)\n",
        "\n",
        "cbs = [unfreeze_cb, checkpoint_best, checkpoint_every_epoch, reduce_lr, early_stop]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVe1M8g5mUih"
      },
      "outputs": [],
      "source": [
        "# sanity checks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRtD832csok7"
      },
      "outputs": [],
      "source": [
        "# for (imgs, dec_in), dec_tgt in train_ds.take(1):\n",
        "#     print(\"imgs shape:    \", imgs.shape)      # (B, 299, 299, 3)\n",
        "#     print(\"dec_in shape:  \", dec_in.shape)    # (B, T)\n",
        "#     print(\"dec_tgt shape: \", dec_tgt.shape)   # (B, T)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdICQnb13du9"
      },
      "outputs": [],
      "source": [
        "# (logits) = model([imgs, dec_in])\n",
        "# print(logits.shape)  # should be (64, 21, vocab_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8xPh7Wtst0X"
      },
      "outputs": [],
      "source": [
        "# i = 0\n",
        "# print(\"First decoder input ids:\", dec_in[i].numpy().tolist())\n",
        "# print(\"First target ids:       \", dec_tgt[i].numpy().tolist())\n",
        "\n",
        "# print(\"Dec_in text:\", tokenizer.sequences_to_texts([dec_in[i].numpy()])[0])\n",
        "# print(\"Dec_tgt text:\", tokenizer.sequences_to_texts([dec_tgt[i].numpy()])[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0haXQBA5UUP"
      },
      "outputs": [],
      "source": [
        "CKPT_BEST = MODEL_DIR + \"/best_val_loss.weights.h5\"\n",
        "\n",
        "# Load pre-trained weights from the best model checkpoint\n",
        "model.load_weights(CKPT_BEST)\n",
        "print(\"Loaded weights from best_val_loss.weights.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78A0Ir-75lr6",
        "outputId": "f03ff696-b7fb-4743-bb24-036fae55f568"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "enc = model.encoder   # the full encoder model\n",
        "# manally unfreeze\n",
        "\n",
        "start = False\n",
        "n = 0\n",
        "\n",
        "for l in enc.layers:\n",
        "    if l.name == \"mixed7\":\n",
        "        start = True\n",
        "    if start:\n",
        "        if isinstance(l, tf.keras.layers.BatchNormalization):\n",
        "            l.trainable = False\n",
        "        else:\n",
        "            l.trainable = True\n",
        "            n += 1\n",
        "\n",
        "print(f\"[Manual unfreeze] {n} layers trainable from 'mixed10' (BN frozen).\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3V6Jn3RKGXC"
      },
      "outputs": [],
      "source": [
        "# Define the number of training epochs\n",
        "EPOCHS = 10\n",
        "\n",
        "# Train the model using the prepared datasets and callbacks\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=cbs,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8Ii0PAu3JNV"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "from tensorflow.keras.applications.inception_v3 import preprocess_input as iv3_preprocess\n",
        "\n",
        "# --- Placeholder Variables ---\n",
        "# Directory containing VizWiz caption data (annotations, images).\n",
        "# This should be a path accessible to your Colab environment or linked from Google Drive.\n",
        "VIZWIZ_CAPTION_DIR = \"/path/to/vizwiz_caption_directory\"\n",
        "\n",
        "# GCS bucket name for storing images and model checkpoints.\n",
        "GCS_BUCKET_NAME = \"your-gcs-bucket-name\"\n",
        "\n",
        "# Local directories where training and validation images will be synced.\n",
        "LOCAL_TRAIN_IMAGES_DIR = \"/path/to/local_train_images\"\n",
        "LOCAL_VAL_IMAGES_DIR   = \"/path/to/local_val_images\"\n",
        "\n",
        "# Local cache paths for TensorFlow datasets.\n",
        "LOCAL_TRAIN_CACHE_PATH = \"/path/to/local_train_cache\"\n",
        "LOCAL_VAL_CACHE_PATH   = \"/path/to/local_val_cache\"\n",
        "\n",
        "# Local path to save BLEU scores before uploading to GCS.\n",
        "LOCAL_BLEU_SCORES_PATH = \"/path/to/bleu_scores.json\"\n",
        "\n",
        "VAL_IMAGE_ROOT = LOCAL_VAL_IMAGES_DIR\n",
        "START_TOKEN = \"startcap\"\n",
        "END_TOKEN   = \"endcap\"\n",
        "\n",
        "# from tokenizer setup\n",
        "# tokenizer, start_id, end_id, vocab_size, max_len_text, df_val_long all exist\n",
        "index_word = tokenizer.index_word    # {id: word}\n",
        "PAD_ID = 0\n",
        "\n",
        "IMG_SIZE = 299\n",
        "\n",
        "def preprocess_image(path):\n",
        "    img = tf.io.read_file(path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize_with_pad(img, IMG_SIZE, IMG_SIZE)\n",
        "    img = tf.cast(img, tf.float32)\n",
        "    return iv3_preprocess(img)\n",
        "\n",
        "def generate_caption_for_path(img_path, model, tokenizer, max_len_text, start_id, end_id):\n",
        "    # 1) Preprocess image\n",
        "    img = preprocess_image(img_path)\n",
        "    img = tf.expand_dims(img, 0)  # (1, 299, 299, 3)\n",
        "\n",
        "    T = max_len_text - 1\n",
        "    seq = [start_id]  # list of token IDs, starting with startcap\n",
        "\n",
        "    for t in range(T):\n",
        "        dec_in = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "            [seq], maxlen=T, padding=\"post\", truncating=\"post\"\n",
        "        )  # (1, T)\n",
        "\n",
        "        logits = model.predict([img, dec_in], verbose=0)  # (1, T, V)\n",
        "\n",
        "        cur_len = len(seq)\n",
        "        step_logits = logits[0, cur_len - 1]  # (V,)\n",
        "        next_id = int(tf.argmax(step_logits).numpy())\n",
        "\n",
        "        if next_id == end_id or next_id == PAD_ID:\n",
        "            break\n",
        "\n",
        "        seq.append(next_id)\n",
        "\n",
        "    # Convert IDs  words (skip special tokens)\n",
        "    words = []\n",
        "    for tid in seq:\n",
        "        if tid in (start_id, end_id, PAD_ID):\n",
        "            continue\n",
        "        word = index_word.get(tid, \"<unk>\")\n",
        "        words.append(word)\n",
        "\n",
        "    return \" \".join(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtpzTvnXEGml"
      },
      "outputs": [],
      "source": [
        "# !pip install nltk\n",
        "import nltk\n",
        "\n",
        "# Import sentence_bleu for individual sentence scoring and SmoothingFunction for handling zero matches\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "# Initialize a smoothing function (Method 1) to avoid division by zero for short or low-matching captions\n",
        "smooth = SmoothingFunction().method1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEs47s8p4Mhb"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def clean_caption_text(text):\n",
        "    # remove start/end tokens from GT captions\n",
        "    return text.replace(START_TOKEN, \"\").replace(END_TOKEN, \"\").strip()\n",
        "\n",
        "# pick a random val image (or use iloc[0])\n",
        "i = random.randrange(len(df_val_long))\n",
        "row = df_val_long.iloc[i]\n",
        "\n",
        "fname = row[\"file_name\"]\n",
        "img_path = os.path.join(VAL_IMAGE_ROOT, fname)\n",
        "\n",
        "print(\"Image file:\", img_path)\n",
        "\n",
        "# predicted caption\n",
        "pred = generate_caption_for_path(\n",
        "    img_path,\n",
        "    model,\n",
        "    tokenizer,\n",
        "    max_len_text,\n",
        "    start_id,\n",
        "    end_id,\n",
        ")\n",
        "print(\"\\nPRED:\", pred)\n",
        "\n",
        "# collect all refs for that image\n",
        "all_caps = df_val_long[df_val_long[\"file_name\"] == fname][\"caption_final\"].tolist()\n",
        "refs_tokens = []\n",
        "\n",
        "for c in all_caps:\n",
        "    c_clean = clean_caption_text(c)\n",
        "    refs_tokens.append(c_clean.split())\n",
        "\n",
        "print(\"\\nREFS:\")\n",
        "for r in refs_tokens:\n",
        "    print(\" -\", \" \".join(r))\n",
        "\n",
        "# BLEU for that one image\n",
        "hyp_tokens = pred.split() if pred.strip() else [\"<empty>\"]\n",
        "single_bleu = sentence_bleu(refs_tokens, hyp_tokens, smoothing_function=smooth)\n",
        "print(f\"\\nSingle-image BLEU: {single_bleu:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82,
          "referenced_widgets": [
            "79fa8bc69e1d4a5ea70abf27d2902240",
            "2a863f082c504a28987e66a08b11e58f",
            "25d6c39f23dd4d6c8208e0b4cea03c56",
            "532b6a2254684c9caaee3b68c71382c7",
            "c6aae3fdd6e54fc6826308e8e2738ec5",
            "78b8a4d88a4e4c4a98904c4a9c4557fe",
            "44d862ca3b004e709fba48bae0671f05",
            "0cdeecfb148d41e4a729c4a32a1ddb94",
            "a64cbcf18aea4109affaea642256713c",
            "6e07f3027bff44c58223a6791e88b311",
            "8728fd2f889c4643b35740a58c6737dc"
          ]
        },
        "id": "f235vzmT4PDb",
        "outputId": "33ebc124-574e-47a0-ab26-56cc58e31bde"
      },
      "outputs": [],
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from tqdm.auto import tqdm  # works nicely in Colab/Notebooks\n",
        "\n",
        "# group df_val_long: file_name -> list of captions\n",
        "groups = df_val_long.groupby(\"file_name\")[\"caption_final\"].apply(list).reset_index()\n",
        "print(\"Num unique val images:\", len(groups))\n",
        "\n",
        "refs = []  # references: list of list-of-token-lists\n",
        "hyps = []  # hypotheses: list of token lists\n",
        "\n",
        "MAX_EVAL = None  # or e.g. 500 if you want a quick run\n",
        "\n",
        "# decide how many we'll process (for tqdm total)\n",
        "total = len(groups) if MAX_EVAL is None else min(len(groups), MAX_EVAL)\n",
        "\n",
        "for idx, row in tqdm(groups.iterrows(), total=total):\n",
        "    if MAX_EVAL is not None and idx >= MAX_EVAL:\n",
        "        break\n",
        "\n",
        "    fname = row[\"file_name\"]\n",
        "    caps  = row[\"caption_final\"]  # list of captions\n",
        "\n",
        "    img_path = os.path.join(VAL_IMAGE_ROOT, fname)\n",
        "\n",
        "    # 1) prediction\n",
        "    pred_sentence = generate_caption_for_path(\n",
        "        img_path,\n",
        "        model,\n",
        "        tokenizer,\n",
        "        max_len_text,\n",
        "        start_id,\n",
        "        end_id,\n",
        "    )\n",
        "    hyp_tokens = pred_sentence.split() if pred_sentence.strip() else [\"<empty>\"]\n",
        "\n",
        "    # 2) references\n",
        "    img_refs = []\n",
        "    for c in caps:\n",
        "        c_clean = clean_caption_text(c)\n",
        "        img_refs.append(c_clean.split())\n",
        "\n",
        "    refs.append(img_refs)\n",
        "    hyps.append(hyp_tokens)\n",
        "\n",
        "print(\"Scored examples:\", len(hyps))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtDhaMG54RkV",
        "outputId": "cdda2657-e0a1-42b4-e793-7ac810e95ae7"
      },
      "outputs": [],
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
        "\n",
        "smooth = SmoothingFunction().method1\n",
        "\n",
        "bleu1 = corpus_bleu(refs, hyps, weights=(1.0, 0.0, 0.0, 0.0), smoothing_function=smooth)\n",
        "bleu2 = corpus_bleu(refs, hyps, weights=(0.5, 0.5, 0.0, 0.0), smoothing_function=smooth)\n",
        "bleu3 = corpus_bleu(refs, hyps, weights=(1/3, 1/3, 1/3, 0.0), smoothing_function=smooth)\n",
        "bleu4 = corpus_bleu(refs, hyps, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smooth)\n",
        "\n",
        "print(f\"BLEU-1: {bleu1:.4f}\")\n",
        "print(f\"BLEU-2: {bleu2:.4f}\")\n",
        "print(f\"BLEU-3: {bleu3:.4f}\")\n",
        "print(f\"BLEU-4: {bleu4:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1DlvLs0KBT8"
      },
      "outputs": [],
      "source": [
        "MODEL_DIR = f\"gs://{GCS_BUCKET_NAME}/vizwiz_models/v1\"\n",
        "\n",
        "import json\n",
        "import os\n",
        "\n",
        "scores = {\n",
        "    \"bleu1\": float(bleu1),\n",
        "    \"bleu2\": float(bleu2),\n",
        "    \"bleu3\": float(bleu3),\n",
        "    \"bleu4\": float(bleu4),\n",
        "    \"num_examples\": len(hyps),\n",
        "}\n",
        "\n",
        "local_path = LOCAL_BLEU_SCORES_PATH\n",
        "\n",
        "with open(local_path, \"w\") as f:\n",
        "    json.dump(scores, f, indent=2)\n",
        "\n",
        "print(\"Saved local:\", local_path)\n",
        "\n",
        "remote_path = f\"{MODEL_DIR}/bleu_scores.json\"\n",
        "!gsutil cp \"{local_path}\" \"{remote_path}\"\n",
        "print(\"Uploaded to:\", remote_path)\n",
        "\n",
        "# used to disconnect from a100 after completion\n",
        "# from google.colab import runtime\n",
        "# runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0cdeecfb148d41e4a729c4a32a1ddb94": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25d6c39f23dd4d6c8208e0b4cea03c56": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0cdeecfb148d41e4a729c4a32a1ddb94",
            "max": 4918,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a64cbcf18aea4109affaea642256713c",
            "value": 4918
          }
        },
        "2a863f082c504a28987e66a08b11e58f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_78b8a4d88a4e4c4a98904c4a9c4557fe",
            "placeholder": "​",
            "style": "IPY_MODEL_44d862ca3b004e709fba48bae0671f05",
            "value": "100%"
          }
        },
        "44d862ca3b004e709fba48bae0671f05": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "532b6a2254684c9caaee3b68c71382c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e07f3027bff44c58223a6791e88b311",
            "placeholder": "​",
            "style": "IPY_MODEL_8728fd2f889c4643b35740a58c6737dc",
            "value": " 4918/4918 [1:27:17&lt;00:00,  1.20s/it]"
          }
        },
        "6e07f3027bff44c58223a6791e88b311": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78b8a4d88a4e4c4a98904c4a9c4557fe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79fa8bc69e1d4a5ea70abf27d2902240": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2a863f082c504a28987e66a08b11e58f",
              "IPY_MODEL_25d6c39f23dd4d6c8208e0b4cea03c56",
              "IPY_MODEL_532b6a2254684c9caaee3b68c71382c7"
            ],
            "layout": "IPY_MODEL_c6aae3fdd6e54fc6826308e8e2738ec5"
          }
        },
        "8728fd2f889c4643b35740a58c6737dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a64cbcf18aea4109affaea642256713c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c6aae3fdd6e54fc6826308e8e2738ec5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
